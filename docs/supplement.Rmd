---
title: "Supplementary Information S5: Participant Characteristics, Statistical Modeling, and Figures"
author: "Pearson et al."
date: "4/27/2018"
header-includes:
  - \usepackage{amsmath, amssymb}
  - \usepackage[labelfont=bf]{caption}
bibliography: supp.bib
output: 
  pdf_document:
    fig_caption: true 
---
\captionsetup[table]{name=Supplementary Table}
\captionsetup[figure]{name=Supplementary Figure}


# Supplementary Tables

## Participant Demographics

```{r message=FALSE, echo=FALSE, warning=FALSE, fig.height=6, fig.cap="Deserved punishment ratings as a function of crime severity. Plotted separately for State (A) and Federal (B) crimes. \\label{demo_table}"}
library(tidyverse)
library(kableExtra)

mturk_demos <- read.csv('../data/combined_data.csv') %>% filter(group=='mturk') %>%
  distinct(uid, .keep_all=TRUE)

# renaming, etc.
cutpts = c(17, 21, 25, 30, 40, 50, 60, 90) # cutpoints for age ranges
mturk_demos$age = cut(mturk_demos$age,cutpts)

levels(mturk_demos$gender) <- c("Male", "Female", "Other")
levels(mturk_demos$race) <- c("American Indian or Alaska Native", 
                              "Asian", 
                              "Black or African American",
                              "Native Hawaiian or other Pacific Islander",
                              "White",
                              "More than one race",
                              "Unknown or do not want to disclose")
levels(mturk_demos$ethnicity) <- c("Hispanic or Latino", 
                                   "Not Hispanic or Latino", 
                                   "Unknown or do not want to disclose")
levels(mturk_demos$education) <- c("Less than high school", 
                                   "Completed high school",
                                   "GED", 
                                   "Some college",
                                   "Associate's Degree", 
                                   "Bachelor's Degree", 
                                   "Master's Degree", 
                                   "Ph.D.", 
                                   "Law degree",
                                   "Other professional degree")
levels(mturk_demos$political_party) <- c("Independent or no party Affiliation", 
                                         "Republican", 
                                         "Democrat", 
                                         "Other", 
                                         "I am not a registered voter")

get_count_tbl <- function(varname){
  id <- "uid"
  Ntot <- dim(mturk_demos)[1]
  tbl <- mturk_demos %>% select_(id, varname) %>% group_by_(varname) %>% 
    summarize(count=n(), percent=round(100 * n()/Ntot, 1))
  names(tbl) <- str_replace_all(names(tbl), "_", " ") %>% tools::toTitleCase()
  return(tbl)
}

kable(get_count_tbl("gender"), caption="Participant Demographics - Gender")
kable(get_count_tbl("age"), caption="Participant Demographics - Age")
kable(get_count_tbl("race"), caption="Participant Demographics - Race")
kable(get_count_tbl("ethnicity"), caption="Participant Demographics - Ethnicity")
kable(get_count_tbl("education"), caption="Participant Demographics - Education")
kable(get_count_tbl("political_party"), caption="Participant Demographics - Political Party")
```

## Testing differences in evidence effects
The values displayed in Tables \ref{group_effects_table} and \ref{outcome_effects_table} are not, strictly speaking, Bayesian hypothesis tests. Here, we simply summarize the posterior probability that the evidence and crime effect means appearing in Figures 3A and 4A are different. We do so by (1) calculating the posteriors for their differences and (2) summarizing our confidence that these posteriors exclude 0 (no difference). More specifically, for two effects with a positive (mean) difference, we calculate the posterior probability that this difference is less than 0. (We do the reverse if the mean difference in effects is negative.) For comparison with a two-tailed test, we subsequently multiply by two. So, for example, a value of 0.02 in the table means that we are 99% confident that the difference in two effect sizes is greater than 0. Nevertheless, these numbers should not be interpreted as false positive rates, only as convenient summaries of posterior distributions.

```{r message=FALSE, echo=FALSE, warning=FALSE, fig.pos="H"}

load('../data/stan_hier_postprocess_pvals.rdata')
pv_table <- group_pv_df %>% mutate_all(funs(cell_spec(., "latex", bold=ifelse(. < 0.05, T, F))))
row.names(pv_table) <- row.names(group_pv_df)

kable(pv_table, format="latex", booktabs=T, escape=F,
      caption="Bayesian ``p-values'' (posterior probability mass exceeding 0) for contrasts 
      between population mean effects. Cf. Figure 3A in main text.\\label{group_effects_table}") %>% 
  kable_styling(full_width=F, latex_options = c("HOLD_position")) %>%
  column_spec(1, width = "16em") %>%
  column_spec(2:7, width = "4.5em")


pv_table <- ratings_pv_df %>% mutate_all(funs(cell_spec(., "latex", bold=ifelse(. < 0.05, T, F))))
row.names(pv_table) <- row.names(ratings_pv_df)

kable(pv_table, format="latex", booktabs=T, escape=F,
      caption="Bayesian ``p-values'' (posterior probability mass exceeding 0) for contrasts 
      between rating type mean effects. Cf. Figure 4A in main text.\\label{outcome_effects_table}") %>% 
  kable_styling(full_width=F, latex_options = c("HOLD_position")) %>%
  column_spec(1, width = "16em") %>%
  column_spec(2:7, width = "4.5em")
```

# Modeling: Case Strength
For the case of a single rating outcome (case strength), we separately model each group of participants as follows: Given a set of
ratings $R$ indexed by $i=1\ldots N$, and an $N \times P$ design matrix $X$ (with columns corresponding to regressors $p=1\ldots P$), we assume for a given observation $i$ corresponding to 
subject $s$ and case $c$:

\begin{align}
  R_i &\sim \left[\mathcal{N}(\theta_i, \sigma^2)\right]^{100}_0 \label{rating}\\
  \theta_i &= X_{i \,\cdot} \cdot \beta_{s(i) c(i) \,\cdot} \label{theta}\\
  \beta_{s c p} &\sim \mathcal{T}_{\nu}(\gamma_{c p}, \tau^2_{c p}) \label{beta}\\
  \gamma_{c p} &\sim \mathcal{T}_{\nu'}(\mu_p, \eta^2_p) \label{gamma}\\
  \mu_p &\sim \mathcal{N}(50, 50) \label{mu} \\
  \eta_p &\sim \mathrm{Ca}^+(0, 50) \\
  \tau_{c p} &\sim \mathrm{Ca}^+(0, 50) \\
  \sigma &\sim \mathrm{Ca}^+(0, 5) \\
  \nu, \nu' &\sim \mathcal{N}^+(0, 100)
\end{align}

That is:

- ratings are generated from a normal distribution censored to lie in the range $[0, 100]$ (\ref{rating})
- linear predictors of ratings are weighted sums of subject-, case-, and regressor-specific effects (\ref{theta})
- $\beta$ effects for each subject are drawn from a case-specific distribution (\ref{beta})
- $\gamma$ effects for each case are themselves drawn from a distribution of effects (\ref{gamma})
- effects at the case and single-subject level are modeled as robuts/fat-tailed, with Student-t distributions (\ref{beta},\ref{gamma})
- variances ($\eta^2$, $\tau^2$, $\sigma^2$) are modeled using weakly informative half-Cauchy priors [@gelman2006prior], 
  while degrees of freedom ($\nu$, $\nu'$) are modeled using weak half-Normal priors

This approach allows for flexible fitting (including estimates of variance) at the regressor, case, and individual levels, while
at the same time leveraging partial pooling to share statistical strength across these levels [@gelman2006data].

# Modeling: Multiple Ratings
For the case in which subjects provide multiple ratings (punishment, case strength, etc.) for a given scenario, we model the resulting
vector of ratings, $R_r$, $r=1\ldots N_r$, similarly

\begin{align}
  R_i &\sim \left[\mathcal{N}(\theta_i, \sigma^2_{r(i)})\right]^{100}_0 \label{rating_m}\\
  \theta_i &= X_{i \,\cdot} \cdot \beta_{s(i) c(i) \,\cdot \,r(i)} \label{theta_m}\\
  \beta_{s c p r} &\sim \mathcal{T}_{\nu}(\gamma_{c p r}, \tau^2_{p r}) \label{beta_m}\\
  \gamma_{c p} &\sim \mathcal{T}_{\nu'}(\mu_p, \Sigma_p) \label{gamma_m}\\
  \Sigma_p &= L_p \cdot \mathrm{diag}(\eta_p)\cdot L_p^\top \\
  \Omega_p = L_p L_p^\top &\sim \mathrm{LKJ}(1) \label{L_m}\\
  \mu_{p r} &\sim \mathcal{N}(50, 50) \label{mu_m} \\
  \eta_{p r} &\sim \mathrm{Ca}^+(0, 50) \\
  \tau_{p r} &\sim \mathrm{Ca}^+(0, 50) \\
  \sigma_r &\sim \mathrm{Ca}^+(0, 5) \\
  \nu, \nu' &\sim \mathcal{N}^+(0, 100)
\end{align}

Here, we have used a "long" or "melted" representation of $R$ in which each index $i$ corresponds to a single observation of a
single rating scale $r(i)$. This allows us to more easily handle missing data in the model fitting procedure (see below). 
The model is almost equivalent to concatenating $N_r$ versions of the first model, one for each rating, 
aside from two key differences: First (\ref{gamma_m}) and (\ref{L_m}) involve a multivariate t-distribution on the population 
effects $\gamma$ specific to 
each case. That is, we allow for covariance among the ratings for each effect at the population level, where the magnitudes of the 
variances are again controlled by $\eta$ and the correlations $\Omega = LL^\top$ are modeled according to the LKJ distribution [@Lewandowski2009-tm]
through their Cholesky factorization (\ref{L_m}).\footnote{Implemented as \texttt{L \textasciitilde{} lkj\_corr\_chol(1)} in Stan.}
Second, in order to more accurately estimate variances in the presence of missing data, we have restricted this model to a single
value of $\tau$ across all cases (for each outcome and regressor) (\ref{beta_m}).

# Model Fitting
We calculated posterior distributions and credible intervals for each variable of interest using Markov Chain Monte Carlo methods
as implemented in the Stan probabilistic programming language [@carpenter2016stan]. Full code for all analyses is available at 
[https://github.com/pearsonlab/legal](https://github.com/pearsonlab/legal). Models were fit by running 4 chains of either 1000 samples (case strength only model) with a thinning fraction of 1 or 2000
samples (multiple outcome models) with a thinning fraction of 2, of which the first half were discarded as burn-in. This resulted 
in 2000 total samples for each variable, for which we report means as well as 95% equal-tailed credible intervals (bounded 
by the 2.5% and 97.5% samples from the distribution). We assessed convergence via effective sample size and the
Gelman-Rubin statistic, for which all runs exhibited $\hat{R} < 1.1$ [@gelman2014bayesian].

# Supplementary Figures


```{r message=FALSE, echo=FALSE, warning=FALSE, fig.height=4, fig.cap="Task learning effect. Comparison of distributions of first and last five ratings given by each participant. \\label{task_learning_effect}"}
source("../ggplot_setup.R")

mturk_ratings <- read.csv('../data/combined_data.csv') %>% filter(group=='mturk')
maxq <- max(unique(mturk_ratings$question))

# filter to first five and last five questions 
first_last <- mturk_ratings %>% filter(question < 5 | maxq - question < 5) %>% 
  mutate(early = factor(question < 5, levels=c(TRUE, FALSE), labels=c("Early", "Late")))

# Density plot early vs late ratings
plt <- ggplot(first_last) +
  geom_density(aes(x=rating, fill=early), alpha=0.5) +
  xlab("Confidence in Guilt") +
  ylab("Density") +
  scale_fill_discrete(name = "Scenario occurrence") +
  th +
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        legend.title=element_text(),
        legend.position=c(0.15, 0.9))
plot(plt)
```

```{r message=FALSE, echo=FALSE, warning=FALSE, fig.height=6, fig.cap="Deserved punishment ratings as a function of crime severity. Plotted separately for State (A) and Federal (B) crimes. \\label{seriousness_by_crime_type}"}
library(tidyverse)
library(grid)
library(gridExtra)
source("../ggplot_setup.R")

load('../data/stan_hier_postprocess_multi.rdata')

# dataframe linking scenarios to seriousness as rank ordered by PS
sc_ranked <- as.factor(c(27, 6, 12, 29, 13, 14, 1, 24, 2, 22, 25,
                                        3, 8, 9, 4, 18, 33, 15, 7, 19, 28, 32, 5, 11, 26, 17,
                                        20, 30, 31, 21, 10, 16, 23))
crime_type <- rep('state', 33)
crime_type[c(1, 14, 28)] <- 'federal'
crime_type <- as.factor(crime_type)

seriousness <- data.frame(seriousness=as.factor(c(1:33)), 
                          scenario=sc_ranked,
                          crime_type=crime_type)

df <- merge(dat, seriousness) %>% filter(rating_type=='rate_punishment') %>% 
  group_by(scenario, seriousness, crime_type) %>% 
  summarise(punish=median(rating), lower=quantile(rating, 0.25), 
            upper=quantile(rating, 0.75))
  

# boxplot punishment rating by seriousness
plt_1 <- ggplot(df %>% filter(crime_type=='state')) +
  geom_pointrange(aes(seriousness, punish, ymin=lower, ymax=upper), color=color_conf) + 
  # geom_smooth(aes(as.numeric(seriousness), punish), color=color_conf, span=0.85, fullrange=TRUE) +
  scale_x_discrete(name='Scenario (rank-ordered by severity)',
                   breaks=c(1:33),
                   labels=seriousness$scenario) + 
  coord_cartesian(ylim=c(0, 100)) +
  labs(title="A. State Crimes", size=rel(3)) +
  ylab("Punishment Rating (points)") +
  th +
  theme(
    axis.text.x = element_text(size=rel(0.75))
    )

plt_2 <- ggplot(df %>% filter(crime_type=='federal')) +
  geom_pointrange(aes(seriousness, punish, ymin=lower, ymax=upper), color=color_conf) + 
  # geom_smooth(aes(as.numeric(seriousness), punish), color=color_conf, span=0.85, fullrange=TRUE) +
  scale_x_discrete(name='Scenario (rank-ordered by severity)',
                   breaks=c(1:33),
                   labels=seriousness$scenario) + 
  coord_cartesian(ylim=c(0, 100)) +
  labs(title="B. Federal Crimes", size=rel(3)) +
  ylab("Punishment Rating (points)") +
  th +
  theme(
    axis.text.x = element_text(size=rel(0.75))
    )
plot(arrangeGrob(plt_1, plt_2))
```

```{r message=FALSE, echo=FALSE, warning=FALSE, fig.width=8, fig.cap="Comparison of evidence and demographic effects for the mTurk sample (cf. Figure 2A). Analyses were performed on a reduced sample of $N=368$ participants with complete demographic information for the variables displayed.  Demographic variables were modeled as additional $\\beta$ effects as in (\\ref{theta}) and (\\ref{beta}). \\label{demo_effects}"}

# based on postprocess_stan_hier_data.R and make_paper_figure_2.R
library(tidyverse)
library(ggplot2)

renamer <- function(x) {
  gsub("\\[\\s*(\\d+)(,\\s*(\\d+))*\\s*\\]", "_\\3\\_\\1", x, perl=TRUE)
}
qprobs <- c(0.025, 0.5, 0.975)

load('../data/stan_model_output_hier_t_mturk_with_demos.rdata')

# get matrix of summary statistics for each variable of interest
ss <- data.frame(rstan::summary(fit, pars=c('mu', 'eta', 'gamma', 'tau', 'sigma'), probs=qprobs)$summary)

# change rownames to make them easy to parse
rownames(ss) <- sapply(rownames(ss), renamer)

# make row names into a column
ss$var <- rownames(ss)
rownames(ss) <- NULL

# make var into separate columns for variable, evidence code, and scenario
ss <- ss %>% separate(var, into=c("variable", "evidence_num", "scenario"))

# make group a character vector so we can merge without worrying about factor levels
preds$group <- as.character(preds$group)

# create a trivial evidence code column
preds$evidence_num <- rownames(preds)

# bind variables columnwise
df <- left_join(ss, preds, by="evidence_num")

fe <- df %>% filter(variable == 'mu', evidence != 'baseline') %>%
      select(mean, evidence, X2.5., X97.5.) %>%
      mutate(evidence=factor(evidence, levels=c("physicalDNA", 
                                                "physicalNon-DNA", 
                                                "witnessYes Witness", 
                                                "historyRelated", 
                                                "historyUnrelated",
                                                "femaleTRUE",
                                                "nonwhiteTRUE",
                                                "hispanicTRUE")))

effects_x_axis <- scale_x_discrete(breaks=c("physicalDNA", 
                                             "physicalNon-DNA", 
                                             "witnessYes Witness", 
                                             "historyRelated", 
                                             "historyUnrelated",
                                             "femaleTRUE",
                                             "nonwhiteTRUE",
                                             "hispanicTRUE"), 
                                     labels=c("DNA \nphysical \nevidence", 
                                              "Non-DNA \nphysical \nevidence",  
                                              "Witness \npresent", 
                                              "Related \nprior crime", 
                                              "Unrelated \nprior crime",
                                              "Female -\nMale",
                                              "Non-white -\nWhite",
                                              "Hispanic -\nNon-hispanic"))
plt_1 <- ggplot(data=fe) +
  geom_pointrange(aes(x=evidence, y=mean, ymin=X2.5., ymax=X97.5.), color=color_conf, size=1.) + 
  effects_x_axis +
  coord_cartesian(ylim=c(-10,40)) +
  ylab("Confidence in Guilt (points)") +
  xlab("Effects") +
  geom_vline(xintercept=1.5, colour='grey') +
  geom_vline(xintercept=2.5, colour='grey') +
  geom_vline(xintercept=3.5, colour='grey') +
  geom_vline(xintercept=4.5, colour='grey') +
  geom_hline(yintercept=0.0, colour='grey') + 
  th +
  theme(
    axis.text.x = element_text(hjust = 0.5, size=rel(1), color='black'))
plt_1
```

```{r message=FALSE, echo=FALSE, warning=FALSE, fig.width=8, fig.cap="Pairwise correlations for baseline effects across all outcome types. Includes both versions of the threat assessment question (cp. Figure 4C). \\label{baseline_correlations}"}
load('../data/stan_hier_postprocess_multi_all.rdata')

outcomes <- levels(effects$outcome)
Nr <- length(outcomes)
releveler <- function(x) {
  # replace outcome numbers with names
  xfac <- factor(x, levels=1:length(outcomes), labels=outcomes)
  
  # now reorder levels so plot looks right
  factor(as.character(xfac), levels=c("rating", "rate_punishment", "rate_outrage", "rate_threat", "rate_threat_2"))
}
corrs <- effects %>% filter(grepl('rho', variable)) %>% 
                     separate(variable, into=c("variable", "outcome1", "outcome2")) %>%
                     mutate_at(c("outcome1", "outcome2"), releveler) %>%
                     filter(evidence=='baseline') %>% 
                     unite(col=contrast, outcome1, outcome2, sep='-') %>%
                     mutate(contrast=factor(contrast, levels=outcome_corr_levels,
                                                      labels=outcome_corr_labels))

plt_3 <- ggplot(data = corrs) +
  geom_hline(yintercept=0, colour='grey') +
  geom_pointrange(aes(x=contrast, y=X50., ymin=X2.5., ymax=X97.5.)) + 
  ylab('Baseline Correlation') +
  xlab('Outcome Pair') +
  coord_cartesian(ylim=c(-1,1)) +
  th +
  theme(
    axis.text.x = element_text(hjust = 0.5, size=rel(0.8), color='black')
  )
plt_3  
```

```{r message=FALSE, echo=FALSE, warning=FALSE, fig.width=8, fig.cap="Correlations between case strength and deserved punishment for all model effects across groups (cp. Figure 4D). \\label{all_group_correlations}"}

load('../data/stan_hier_postprocess_multi.rdata')

plt_4 <- ggplot(data=(effects %>% filter(variable=='rho'))) +
  geom_hline(yintercept=0, colour='grey') +
  geom_pointrange(aes(x=evidence, y=X50., ymin=X2.5., ymax=X97.5., color=group), 
                         position=position_dodge(width = 0.5)) + 
  xlab('Evidence') + ylab('\nConfidence in Guilt /\nPunishment Correlation') +
  group_color_scale +
  evidence_plus_baseline_x_axis +
  th + 
  theme(
    axis.text.x = element_text(hjust = 0.5, size=rel(1), color='black'),
    legend.position=c(0.15, 0.2),
    legend.box.background = element_rect(fill=alpha("white", 0.75), color=NULL, linetype=NULL)
  )
plt_4
```

```{r message=FALSE, echo=FALSE, warning=FALSE, fig.width=8, fig.cap="Comparison of sources of model variance. Points indicate posterior means of variance parameters. Lines indicate 95% credible intervals. Boxplots indicate variability of variance parameters across scenarios. Columns correspond to model variance parameters $\\tau^2$, $\\eta^2$, and $\\sigma^2$, respectively. \\label{model_variance_comparison}"}
############### Panel 5: Variance Comparison #################################
load('../data/stan_hier_postprocess.rdata')
variance_comparison <- effects %>%
  filter(variable %in% c('eta', 'tau', 'sigma'), (evidence=='baseline') | (variable == 'sigma')) %>%
  select(X2.5., X50., X97.5., variable, scenario, group) %>%
  mutate(variable = factor(variable, levels=c('eta', 'tau', 'sigma')))

plt_5 <- ggplot() +
  geom_pointrange(data=variance_comparison %>% filter(variable %in% c('eta', 'sigma')),
                  aes(x=variable, y=X50., ymin=X2.5., ymax=X97.5., color=group),
                  position=position_dodge(width=0.5), size=1) +
  geom_boxplot(data=variance_comparison %>% filter(variable=='tau'),
               aes(x=variable, y=X50., color=group), lwd=1, fatten=1, 
               position=position_dodge(width=0.85)) +
  variance_x_axis +
  group_color_scale +
  ylim(0, 50) +
  ylab("Standard Deviation (points)") + 
  xlab("Variance") +
  labs(title="E", size=rel(3)) +
  th +
  theme(
    axis.text.x = element_text(hjust = 0.5, size=rel(1), color='black'),
    legend.position = c(0.8, 0.8)
  )
plt_5
```

\clearpage

# References